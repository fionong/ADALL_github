{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP32w0joRt5h0dp1/LQcxMp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fionong/ADALL_github/blob/main/Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Core libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualisation\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from openai import OpenAI\n",
        "\n",
        "# Modelling and preprocessing\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "import kagglehub\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "path = kagglehub.dataset_download(\"devansodariya/student-performance-data\")\n",
        "print(\"Downloaded to:\", path)\n",
        "print(os.listdir(path))\n",
        "\n",
        "df = pd.read_csv(os.path.join(path, \"student_data.csv\"))\n",
        "\n",
        "from io import StringIO\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load key from Google Colab Secrets\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=api_key,\n",
        ")\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-5-mini\",\n",
        "    instructions=(\n",
        "        \"You are a senior Python data scientist. \"\n",
        "        \"Respond ONLY with executable Python code. \"\n",
        "        \"Do not include explanations or markdown.\"\n",
        "    ),\n",
        "    input=(\n",
        "        \"Write Python code using pandas to:\\n\"\n",
        "        \"1. Display the shape of a DataFrame\\n\"\n",
        "        \"2. Show column dtypes\\n\"\n",
        "        \"3. Generate summary statistics\\n\"\n",
        "        \"4. Check for missing values\\n\\n\"\n",
        "        \"Assume the DataFrame is named df.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5P64lOR5gepR",
        "outputId": "7f087657-ef5e-4b42-cc5f-6af88545940f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import pandas as pd\n",
            "import numpy as np\n",
            "\n",
            "# Assumes DataFrame named df exists in the environment.\n",
            "\n",
            "# Basic info\n",
            "n_rows, n_cols = df.shape\n",
            "shape_info = f\"Shape: {n_rows} rows, {n_cols} columns\"\n",
            "\n",
            "# Column dtypes\n",
            "dtypes = df.dtypes\n",
            "\n",
            "# Summary statistics (include all to capture object/category stats)\n",
            "try:\n",
            "    summary_stats = df.describe(include='all', datetime_is_numeric=False).transpose()\n",
            "except Exception:\n",
            "    # fallback if any issue with describe\n",
            "    summary_stats = df.describe(include='all').transpose()\n",
            "\n",
            "# Missing values\n",
            "missing_count = df.isna().sum()\n",
            "if n_rows > 0:\n",
            "    missing_pct = (missing_count / n_rows * 100).round(2)\n",
            "else:\n",
            "    missing_pct = pd.Series(0, index=missing_count.index)\n",
            "\n",
            "missing_summary = pd.concat([missing_count, missing_pct], axis=1)\n",
            "missing_summary.columns = ['missing_count', 'missing_pct']\n",
            "missing_summary = missing_summary.sort_values(by='missing_pct', ascending=False)\n",
            "\n",
            "# Duplicates and constant columns\n",
            "duplicate_count = int(df.duplicated().sum())\n",
            "unique_counts = df.nunique(dropna=False)\n",
            "constant_columns = unique_counts[unique_counts <= 1].index.tolist()\n",
            "\n",
            "# Column type groups\n",
            "numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
            "categorical_cols = df.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
            "datetime_cols = df.select_dtypes(include=['datetime', 'datetimetz']).columns.tolist()\n",
            "\n",
            "# High cardinality and top uniques\n",
            "top_unique = unique_counts.sort_values(ascending=False).head(10)\n",
            "\n",
            "# Numeric skewness (if any numeric cols)\n",
            "skew_info = None\n",
            "if numeric_cols:\n",
            "    skewness = df[numeric_cols].skew(numeric_only=True).abs().sort_values(ascending=False)\n",
            "    skew_info = skewness.head(10)\n",
            "\n",
            "# Notable issues detection\n",
            "notable_issues = []\n",
            "missing_columns = missing_summary[missing_summary['missing_count'] > 0]\n",
            "if not missing_columns.empty:\n",
            "    n_missing_cols = missing_columns.shape[0]\n",
            "    top_missing = missing_summary.head(5)\n",
            "    notable_issues.append(f\"{n_missing_cols} columns contain missing values; top by % missing:\\n{top_missing.to_string()}\")\n",
            "    very_high_missing = missing_summary[missing_summary['missing_pct'] > 90]\n",
            "    if not very_high_missing.empty:\n",
            "        notable_issues.append(f\"Columns with >90% missing: {', '.join(very_high_missing.index.tolist())}\")\n",
            "if duplicate_count > 0:\n",
            "    notable_issues.append(f\"{duplicate_count} duplicate rows detected.\")\n",
            "if constant_columns:\n",
            "    notable_issues.append(f\"Constant columns (single unique value): {', '.join(constant_columns)}\")\n",
            "high_cardinality = top_unique[top_unique > max(50, 0.5 * n_rows if n_rows>0 else 0)]\n",
            "if not high_cardinality.empty:\n",
            "    notable_issues.append(f\"High-cardinality columns (unique values):\\n{high_cardinality.to_string()}\")\n",
            "if skew_info is not None and not skew_info.empty:\n",
            "    high_skew = skew_info[skew_info > 5]  # extreme skew threshold\n",
            "    if not high_skew.empty:\n",
            "        notable_issues.append(f\"Highly skewed numeric columns (abs(skew) > 5):\\n{high_skew.to_string()}\")\n",
            "\n",
            "if not notable_issues:\n",
            "    notable_issues_text = \"No major issues detected (no missing values, no duplicates, no constant or high-cardinality columns found).\"\n",
            "else:\n",
            "    notable_issues_text = \"\\n\\n\".join(notable_issues)\n",
            "\n",
            "# Build a concise text-based payload describing the dataset\n",
            "dataset_summary = f\"\"\"\n",
            "Dataset Summary\n",
            "---------------\n",
            "{shape_info}\n",
            "Total columns: {n_cols}\n",
            "Numeric columns: {len(numeric_cols)}\n",
            "Categorical/Boolean columns: {len(categorical_cols)}\n",
            "Datetime columns: {len(datetime_cols)}\n",
            "\n",
            "Column dtypes (sample):\n",
            "{dtypes.head(20).to_string()}\n",
            "\n",
            "Top 10 columns by unique value count:\n",
            "{top_unique.to_string()}\n",
            "\n",
            "Missing Value Overview (top 10 by % missing):\n",
            "{missing_summary.head(10).to_string()}\n",
            "\n",
            "Duplicate rows: {duplicate_count}\n",
            "Constant columns: {', '.join(constant_columns) if constant_columns else 'None'}\n",
            "\n",
            "Notable Issues:\n",
            "{notable_issues_text}\n",
            "\n",
            "Brief column type lists:\n",
            "Numeric columns (up to 20 shown): {', '.join(numeric_cols[:20])}\n",
            "Categorical/Boolean columns (up to 20 shown): {', '.join(categorical_cols[:20])}\n",
            "Datetime columns (up to 20 shown): {', '.join(datetime_cols[:20])}\n",
            "\"\"\"\n",
            "\n",
            "# Display requested outputs\n",
            "print(\"=\"*80)\n",
            "print(\"1) Shape\")\n",
            "print(shape_info)\n",
            "print(\"=\"*80)\n",
            "print(\"2) Column dtypes (all columns):\")\n",
            "print(dtypes.to_string())\n",
            "print(\"=\"*80)\n",
            "print(\"3) Summary statistics (describe):\")\n",
            "print(summary_stats.to_string(max_rows=200, max_cols=200))\n",
            "print(\"=\"*80)\n",
            "print(\"4) Missing values per column (count and %):\")\n",
            "print(missing_summary.to_string())\n",
            "print(\"=\"*80)\n",
            "print(\"Dataset description payload:\")\n",
            "print(dataset_summary)\n",
            "\n",
            "# Expose the summary string for potential programmatic use\n",
            "dataset_description_payload = dataset_summary.strip()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.responses.create(\n",
        "    model=\"gpt-5-mini\",\n",
        "    instructions=(\n",
        "        \"You are a senior Python data scientist. \"\n",
        "        \"Respond ONLY with executable Python code. \"\n",
        "        \"Do not include explanations or markdown.\"\n",
        "    ),\n",
        "    input=(\n",
        "        \"Write Python code using  to:\\n\"\n",
        "        \"Create a text-based payload that clearly describes the dataset. The description should summarise the dataset structure, key columns, size, and any notable data issues.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "id": "tZnZw23Fgqcl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}